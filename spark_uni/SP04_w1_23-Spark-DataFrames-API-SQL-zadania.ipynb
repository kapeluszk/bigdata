{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Sprawdzenie kernela \n\nNa początek sprawdź czy silnik wykonawczy Twojego notatnika to PySpark. \nMógłby on być po prostu interpreterem Pythona, jednak wówczas zmienne kontekstu musielibyśmy tworzyć samodzielnie.\n\nSprawdź, czy obiekt kontekstu jest dostępny. W przypadku *Spark SQL* jest to `SparkSession`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "spark"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Dzięki powyższej informacji dowiedzieliśmy się nie tylko w jakim trybie został uruchomiony Spark obsługujący nasze polecenia, w jakiej jest wersji, ale także czy obsługuje funkcjonalność platformy Hive.\n\nDowiedz się także pod jakim użytkownikiem działamy w ramach tego notatnika."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%sh \nwhoami"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Czas na nasze właściwe zadania. \n\nW razie potrzeby korzystaj z https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html\n\n# 20 Years of Games"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**7. Zaczytaj do zmiennej gameInfosDF zawartość pliku ign.csv**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "username = \"jankiewicz_krzysztof\" # UWAGA! ustaw zmienną username na poprawną wartość\n\ngameInfosDF=spark.read.\\\n    option(\"inferSchema\", \"true\").\\\n    csv(f\"/user/{username}/ign.csv\", header=True).cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**8. Wyświetl schemat zmiennej gameInfosDF**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "gameInfosDF.printSchema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": "Możesz także po prostu przyglądnąć się jej kolumnom"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "gameInfosDF.columns"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": "Zobaczmy też trzy pierwsze wiersze. Zróbmy to na kilka sposobów. \n\n* Na początek metoda `show()`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "gameInfosDF.limit(3).show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": "Przetwarzane dane mogą być duże. Wyniki natomiast z reguły są znacznie mniejsze, to pozwala nam je (o ile znamy ich wielkość) przekonwertować do obiektów `pandas DataFrame` i dzięki temu przedstawić w przyjaźniejszej postaci.\n* metoda `toPandas()`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "gameInfosDF.limit(3).toPandas()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": "Za pomocą parametru konfiguracyjnego `spark.sql.repl.eagerEval.enabled` naszego kontekstu, również możemy \nułatwić sobie wgląd w zawartość naszych wyników. Warto także ustawić parametr aby kontrolować liczbę pobieranych w ten sposób wierszy (tak, w razie niedoszacowania wyniku)\n* parametr `spark.sql.repl.eagerEval.enabled`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\nspark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 3)\ngameInfosDF"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": "Wykorzystuj powyższe, aby móc podglądać uzyskiwane wyniki"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": " \n**9. Na początek coś prostego. \nWyświetl trzy najlepiej ocenione gry wydane w roku 2016 na platformę PC.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "# tu wprowadź swoje rozwiazanie\n",
    "gameInfosDF.where(col(\"platform\")==\"PC\").\\\n",
    "    where(col(\"release_year\")==\"2016\").\\\n",
    "    orderBy(col(\"score\").desc()).\\\n",
    "    limit(3).select(\"title\",\"score\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**10. Określ dla każdej oceny opisowej (score_phrase) minimalną i \nmaksymalną ocenę liczbową. Wyniki posortuj\nrosnąco pod względem minimalnej oceny liczbowej.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 20)\n",
    "# tu wprowadź swoje rozwiazanie\n",
    "gameInfosDF.groupBy(\"score_phrase\").\\\n",
    "    agg(\n",
    "        max(col(\"score\")).alias(\"max_score\"),\n",
    "        min(col(\"score\")).alias(\"min_score\")\n",
    "    ).\\\n",
    "    select(\"score_phrase\",\"max_score\",\"min_score\").\\\n",
    "    orderBy(col(\"min_score\").asc()).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**11. To może coś trudniejszego. Wyznacz liczbę oraz średnią ocenę gier wydawanych w poszczególnych latach\npocząwszy od roku 2000 na poszczególne platformy. Nie analizuj wszystkich platform – ogranicz je tylko do\ntych, dla których liczba wszystkich recenzji gier biorąc pod uwagę wszystkie lata przekroczyła 500.**\n\n*Uwaga: Klasycznie odwołalibyśmy się do źródłowego zboru danych dwa razy. Raz aby wyznaczyć popularne platformy, a następnie aby wyznaczyć ostateczny wynik. \nKorzystając z funkcji analitycznych możesz to zadanie rozwiązać sięgając do źródłowych danych tylko raz.*\n\n**Rozwiąż to zadanie na dwa sposoby:**\n\na. Za pomocą DataFrame API\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "from pyspark.sql.window import Window\n# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "b. Za pomocą SQL (po zarejestrowaniu źródeł danych jako tymczasowych perspektyw)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "gameInfosDF.createOrReplaceTempView(\"gameinfos\")\n# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**12. Jeśli masz swoją ulubioną serię gier (https://pl.wikipedia.org/wiki/Kategoria:Serie_gier_komputerowych)\nzobacz jakie średnie oceny zdobyły poszczególne pozycje z tej serii. Wyniki posortuj chronologicznie.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**13. (opcjonalne) Porównaj ze sobą gry wchodzące w skład wybranych serii gier wchodzących w skład 20\nnajlepszych serii wg Guinessa (lista z 2010 roku). W związku z tym, że gry nie są wydawane co roku, pogrupuj\ndane w przedziały o długości 5 lat.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "# brudnopis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": " \n# MondialDB – DataFrames"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**14. Na początku do zmiennych `citiesDF`, `countriesDF` załaduj odpowiednio dane z plików\n`mondial.cities.json`, `mondial.countries.json`**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "citiesDF = spark.read.json(f\"/user/{username}/mondial.cities.json\").cache()\ncountriesDF = spark.read.json(f\"/user/{username}/mondial.countries.json\").cache()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**15. Zapoznaj się z ich strukturą. Zwróć uwagę na występujące typy array.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "citiesDF.printSchema()\ncountriesDF.printSchema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**16. Zanim zaczniesz realizować zadania, zapoznaj się ze funkcją `explode`, która nadaje się świetnie do pracy z tablicami i ich rozpłaszczania.**\n\n**Przykładowe zapytanie:**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "countriesDF.where(\"name = 'Poland'\").\\\n            select(col(\"name\"), explode(col(\"population\")).alias(\"pop_in_years\"))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Zwróć uwagę także na inne funkcje z tej rodziny jak: `explode_outer`, `posexplode`, `posexplode_outer`\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n\n\nWszystkie zadania wykonaj korzystając *DataFrame API*. Nie korzystaj z SQL."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**17. Oblicz sumę ludności wszystkich Państw na rok 2010. \nW sytuacji gdy w danym kraju nie przeprowadzono\nbadania w roku 2010 wykorzystaj najnowsze z badań wcześniejszych.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "from pyspark.sql.window import Window\n# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**18. Było ciężko? Nie wierzę.**\n\n**Teraz już będzie z górki. Podaj nazwy i gęstość zaludnienia trzech krajów o największej gęstości zaludnienia w roku 2010.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n**19. Podaj trzy kraje o największym procencie ludności żyjącym w miastach powyżej 50 000 mieszkańców w roku\n2010.**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "# tu wprowadź swoje rozwiazanie\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "No cóż, dane dotyczące ludności w miastach są zapewne nowsze niż z 2010 roku.\nNa marginesie, zarówno Melilla jak i Ceuta to hiszpańskie miasta, afrykańskie eksklawy położone na terytorium\nMaroka. Oba liczą ponad 70 tyś mieszkańców i oba posiadają autonomię (uzyskaną jednocześnie w marcu 1995 roku)\ndlatego znalazły się w naszym zestawieniu.\nA co to takiego eksklawy i czy enklawa jest tym samym, to już możesz przeczytać samodzielnie np. tu:\nhttps://pl.wikipedia.org/wiki/Eksklawa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": "# brudnopis"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "name": "Spark – DataFrames"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
